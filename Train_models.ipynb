{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training SVD and doc2vec Models (Phase 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input:\n",
    "   Various parameters found in EDA.\n",
    "#### Output:\n",
    "   Training and saving models \n",
    "#### Algorithm:\n",
    "   a) Train tfidfVectorizer and SVD with components with variance_ratio > 0.95. <br>\n",
    "   b) Train doc2vec from gensim for topic modelling.<br>\n",
    "   c) SVD is used to penalise the queries outside of domain. <br>\n",
    "   d) Doc2vec is used for ranking documents based on scores.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.tokenize import word_tokenize\n",
    "import joblib\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.test.utils import get_tmpfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_models():\n",
    "    \n",
    "    def __init__(self, min_count=25, n_components=100, batch_size=10):\n",
    "        self.n_components = n_components\n",
    "        self.batch_size = batch_size\n",
    "        self.df = pd.read_csv(\"dataset_processed.csv\", names=[\"files\", \"keywords\", \"weights\"])\n",
    "        self.df.dropna(inplace=True)\n",
    "        self.df.reset_index(inplace=True)\n",
    "        self.tf = self.train_vectorizer(min_count=min_count)\n",
    "        self.svd, self.svd_feature_matrix = self.train_svd()\n",
    "        self.dv, self.doc2vec_feature_matrix = self.train_doc2vec(min_count=min_count)\n",
    "        \n",
    "    def save_models(self):\n",
    "        joblib.dump(self.tf, \"./model/tfidf_model.pkl\")\n",
    "        joblib.dump(self.svd, \"./model/svd_model.pkl\")\n",
    "        joblib.dump(self.svd_feature_matrix, \"./model/lsa_embeddings.pkl\")\n",
    "        self.dv.save(\"./model/doc2vec_model\")\n",
    "        joblib.dump(self.doc2vec_feature_matrix, \"./model/doc2vec_embeddings.pkl\")\n",
    "        joblib.dump(self.df, \"./model/dataset.pkl\")\n",
    "        print(\"All Models saved\")\n",
    "        \n",
    "        \n",
    "    def train_vectorizer(self, min_count):\n",
    "        tf = TfidfVectorizer(analyzer=\"word\", min_df=min_count, ngram_range=(1, 3), stop_words=\"english\")\n",
    "        print(\"Tfidf-trained\")\n",
    "        return tf.fit(self.df[\"keywords\"])\n",
    "    \n",
    "    \n",
    "    def train_svd(self):\n",
    "        tfidf_matrix = self.tf.transform(self.df[\"keywords\"])\n",
    "        svd = TruncatedSVD(n_components=1356)\n",
    "        print(\"Training SVD\")\n",
    "        latent_matrix = svd.fit_transform(tfidf_matrix)\n",
    "        svd_feature_matrix = pd.DataFrame(latent_matrix, index=self.df[\"files\"])\n",
    "        print(\"SVD and SVD matrix trained\")\n",
    "        return svd, svd_feature_matrix\n",
    "    \n",
    "    def train_doc2vec(self, min_count):\n",
    "        print(\"Trianing doc2vec\")\n",
    "        tagged_data = [TaggedDocument(words=word_tokenize(\"\".join(_d)), tags=[i]) for i, _d in enumerate(self.df[\"keywords\"])]\n",
    "        doc2vec = Doc2Vec(vector_size=62, hs=1, sample=0,  min_count=min_count, epochs=10, seed=0, window=3, dm=1)\n",
    "        doc2vec.build_vocab(tagged_data)\n",
    "        doc2vec.train(tagged_data, total_examples=doc2vec.corpus_count, epochs=10)\n",
    "        doc2vec_feature_matrix = pd.DataFrame(doc2vec.docvecs.vectors_docs, index=self.df[\"files\"])\n",
    "        fname = get_tmpfile(\"models/doc2vec_model\")\n",
    "        print(\"Doc2vec and Doc2vec matrix trained\")\n",
    "        return doc2vec, doc2vec_feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf-trained\n",
      "Training SVD\n",
      "SVD and SVD matrix trained\n",
      "Trianing doc2vec\n",
      "Doc2vec and Doc2vec matrix trained\n",
      "CPU times: user 2min 57s, sys: 18.4 s, total: 3min 15s\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_models = Train_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Models saved\n",
      "CPU times: user 1.48 s, sys: 1.16 s, total: 2.65 s\n",
      "Wall time: 6.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_models.save_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of phase 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
