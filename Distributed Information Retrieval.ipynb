{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import get_client\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from dask_ml.metrics.pairwise import euclidean_distances\n",
    "from bert_serving.client import BertClient\n",
    "import string\n",
    "import re\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_client(\"tcp://127.0.0.1:49384\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords_list = client.get_dataset('stop_words')\n",
    "bert = BertClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = text.replace(\",\", \" \").replace(\":\", \" \")\n",
    "    text = str(text).lower()\n",
    "    printable = set(string.printable)\n",
    "    text = \"\".join(list(filter(lambda x: x in printable, text)))\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in string.punctuation]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return \" \".join([word for word in tokens if word not in stopwords_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substract_words(query, words_list):\n",
    "    tokens = word_tokenize(query)\n",
    "    return \" \".join([token for token in tokens if token not in words_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_retrieval(query):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Algorithm:\n",
    "    \n",
    "    Step 1: Filter and Preprocess the query.\n",
    "    Step 2: Initialize result = []\n",
    "    Step 3: Read all the models.\n",
    "    Step 4: Disproportionate the query into project, libs, frameworks, dbs, keywords.\n",
    "    Step 3: Add the documents in the resultset if query keywords found in project title.\n",
    "    Step 4: If more than 70% of the query found in project title and technologies used in project then find cosine_similarity\n",
    "            between the query and the feature matrix. Return files with minimum distance.\n",
    "    Step 5: Else\n",
    "            a) Predict Topic from LDA model get documents with similar topics.\n",
    "            b) Find cosine_similarity with bert_embeddings with given query.\n",
    "            c) Sort and aggregate the results and return documents.\n",
    "    Step 6: Calculate Precision, Recall, F-Score for both scenario to tune the parameters.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1\n",
    "    query = clean(query)\n",
    "    query_len = len(word_tokenize(query))\n",
    "    \n",
    "    if query_len == 0:\n",
    "        return\n",
    "    \n",
    "    \n",
    "    # Step 2\n",
    "    result = pd.DataFrame()\n",
    "    query_data = {\n",
    "        \"project\": 0,\n",
    "        \"libs\": 0,\n",
    "        \"frameworks\": 0,\n",
    "        \"dbs\": 0\n",
    "    }\n",
    "    \n",
    "    # Step 3: \n",
    "    tf_projects = client.get_dataset('tf_projects')\n",
    "    tf_libs = client.get_dataset('tf_libs')\n",
    "    tf_frameworks = client.get_dataset('tf_frameworks')\n",
    "    tf_dbs = client.get_dataset('tf_dbs')\n",
    "    \n",
    "    # Step 4:\n",
    "    tf_project_query = tf_projects.transform([query]).toarray()\n",
    "    tf_project_query = np.nonzero(tf_project_query)[1]\n",
    "    query_data['project'] = len(tf_project_query) / query_len\n",
    "    \n",
    "    tf_libs_query = tf_libs.transform([query]).toarray()\n",
    "    tf_libs_query = np.nonzero(tf_libs_query)[1]\n",
    "    query_data['libs'] = len(tf_libs_query) / query_len\n",
    "    if query_data['libs'] > 0:\n",
    "        tf_libs_query_words = np.array(tf_libs.get_feature_names())[tf_libs_query]\n",
    "        query = substract_words(query, tf_libs_query_words)    \n",
    "    \n",
    "    tf_frameworks_query = tf_frameworks.transform([query]).toarray()\n",
    "    tf_frameworks_query = np.nonzero(tf_frameworks_query)[1]\n",
    "    query_data['frameworks'] = len(tf_frameworks_query) / query_len\n",
    "    if query_data['frameworks'] > 0:\n",
    "        tf_frameworks_query_words = np.array(tf_frameworks.get_feature_names())[tf_frameworks_query]\n",
    "        query = substract_words(query, tf_frameworks_query_words)\n",
    "    \n",
    "    tf_dbs_query = tf_dbs.transform([query]).toarray()\n",
    "    tf_dbs_query = np.nonzero(tf_dbs_query)[1]\n",
    "    query_data['dbs'] = len(tf_dbs_query) / query_len\n",
    "    if query_data['dbs'] > 0:\n",
    "        tf_dbs_query_words = np.array(tf_dbs.get_feature_names())[tf_dbs_query]\n",
    "        query = substract_words(query, tf_dbs_query_words)\n",
    "    \n",
    "    # Step 5:\n",
    "    if query_data['libs'] > 0:\n",
    "        tf_libs_matrix = client.get_dataset('tf_libs_matrix')\n",
    "        tf_libs_distance = dd.from_dask_array(da.from_array(euclidean_distances(tf_libs.transform([query]).toarray(), tf_libs_matrix.drop([\"project\", \"file\"], axis=1).to_dask_array()).compute().reshape(-1, 1)), columns=[\"distance\"])\n",
    "        tf_libs_distance['project'] = tf_libs_matrix['project']\n",
    "        tf_libs_distance['file'] = tf_libs_matrix['file']\n",
    "        tf_libs_distance = tf_libs_distance.nsmallest(5, \"distance\").compute()[['project', 'file']]\n",
    "        result = pd.concat([result, tf_libs_distance])\n",
    "        \n",
    "    \n",
    "    if query_data['frameworks'] > 0:\n",
    "        tf_frameworks_matrix = client.get_dataset('tf_frameworks_matrix')\n",
    "        tf_frameworks_distance = dd.from_dask_array(da.from_array(euclidean_distances(tf_frameworks.transform([query]).toarray(), tf_frameworks_matrix.drop([\"project\", \"file\"], axis=1).to_dask_array()).compute().reshape(-1, 1)), columns=[\"distance\"])\n",
    "        tf_frameworks_distance['project'] = tf_frameworks_matrix['project']\n",
    "        tf_frameworks_distance['file'] = tf_frameworks_matrix['file']\n",
    "        tf_frameworks_distance = tf_frameworks_distance.nsmallest(5, \"distance\").compute()[['project', 'file']]\n",
    "        result = pd.concat([result, tf_frameworks_distance])\n",
    "        \n",
    "        \n",
    "    if query_data['dbs'] > 0:\n",
    "        tf_dbs_matrix = client.get_dataset('tf_dbs_matrix')\n",
    "        tf_dbs_distance = dd.from_dask_array(da.from_array(euclidean_distances(tf_projects.transform([query]).toarray(), tf_dbs_matrix.drop([\"project\", \"file\"], axis=1).to_dask_array()).compute().reshape(-1, 1)), columns=[\"distance\"])\n",
    "        tf_dbs_distance['project'] = tf_dbs_matrix['project']\n",
    "        tf_dbs_distance['file'] = tf_dbs_matrix['file']\n",
    "        tf_dbs_distance = tf_dbs_distance.nsmallest(5, \"distance\").compute()[['project', 'file']]\n",
    "        result = pd.concat([result, tf_dbs_distance])\n",
    "        \n",
    "        \n",
    "    if query_data['project'] > 0:\n",
    "        tf_project_matrix = client.get_dataset('tf_project_matrix')\n",
    "        tf_project_distance = dd.from_dask_array(da.from_array(euclidean_distances(tf_projects.transform([query]).toarray(), tf_project_matrix.drop([\"project\", \"file\"], axis=1).to_dask_array()).compute().reshape(-1, 1)), columns=[\"distance\"])\n",
    "        tf_project_distance['project'] = tf_project_matrix['project']\n",
    "        tf_project_distance['file'] = tf_project_matrix['file']\n",
    "        tf_project_distance = tf_project_distance.nsmallest(5, \"distance\").compute()[['project', 'file']]\n",
    "        result = pd.concat([result, tf_project_distance])\n",
    "        \n",
    "        \n",
    "    if len(word_tokenize(query)) > 0:\n",
    "        \n",
    "        \n",
    "        id2word = client.get_dataset('id2word')\n",
    "        lda_model = client.get_dataset('lda_model')\n",
    "        context_based_feature_matrix = client.get_dataset('context_based_feature_matrix')\n",
    "        \n",
    "        corpus = id2word.doc2bow(word_tokenize(query))\n",
    "        topics_distribution = dict(lda_model[corpus][0])\n",
    "        topic = sorted(topics_distribution.items(), key=lambda x: x[1], reverse=True)[0][0]\n",
    "        context_based_topic_result = context_based_feature_matrix[context_based_feature_matrix['Dominant_Topic'] == topic]\n",
    "\n",
    "        query_vec = bert.encode([query])\n",
    "        context_based_result = dd.from_dask_array(da.from_array(euclidean_distances(query_vec, context_based_topic_result.drop([\"project\", \"file_y\", \"keyword\", \"Dominant_Topic\", \"Topic_Perc_Contrib\"], axis=1).to_dask_array()).compute()).reshape(-1, 1), columns=[\"distance\"])\n",
    "        context_based_result['project'] = context_based_topic_result['project']\n",
    "        context_based_result['file'] = context_based_topic_result['file_y']\n",
    "        context_based_result = context_based_result.nsmallest(25, 'distance').groupby(['project', 'file']).agg({'distance': sum}).reset_index().nsmallest(10, 'distance').compute()[['project', 'file']]\n",
    "        result = pd.concat([result, context_based_result])\n",
    "    \n",
    "    result = result.drop_duplicates()\n",
    "    \n",
    "    return result\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.98 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>disease prediction using machine learning</td>\n",
       "      <td>./reports_doc/Disease prediction using machine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>suspect analysis using machine learning of the of</td>\n",
       "      <td>./reports_doc/Suspect Analysis using Machine L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>resume analysis through machine learning</td>\n",
       "      <td>./reports_doc/Resume analysis through machine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>stock price prediction using machine learning</td>\n",
       "      <td>./reports_doc/Group.No_23.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>android malware detection using machine learning</td>\n",
       "      <td>./reports_doc/Android Malware Detection Using ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>audit and compliance in service management usi...</td>\n",
       "      <td>./reports_doc/AUDIT AND COMPLIANCE IN SERVICE_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               project  \\\n",
       "109          disease prediction using machine learning   \n",
       "389  suspect analysis using machine learning of the of   \n",
       "360           resume analysis through machine learning   \n",
       "187      stock price prediction using machine learning   \n",
       "43    android malware detection using machine learning   \n",
       "0    audit and compliance in service management usi...   \n",
       "\n",
       "                                                  file  \n",
       "109  ./reports_doc/Disease prediction using machine...  \n",
       "389  ./reports_doc/Suspect Analysis using Machine L...  \n",
       "360  ./reports_doc/Resume analysis through machine ...  \n",
       "187                      ./reports_doc/Group.No_23.pdf  \n",
       "43   ./reports_doc/Android Malware Detection Using ...  \n",
       "0    ./reports_doc/AUDIT AND COMPLIANCE IN SERVICE_...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "information_retrieval(\"artificial intelligence and machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
