{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import get_client\n",
    "from bert_serving.client import BertClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_client(\"tcp://127.0.0.1:62157\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertClient()\n",
    "lemmatizer = client.get_dataset('lemmatizer')\n",
    "stopwords_list = client.get_dataset('stop_words')\n",
    "word_tokenize = client.get_dataset('word_tokenize')\n",
    "string = client.get_dataset('string')\n",
    "re = client.get_dataset('re')\n",
    "euclidean_distances = client.get_dataset('euclidean_distances')\n",
    "da = client.get_dataset('da')\n",
    "dd = client.get_dataset('dd')\n",
    "np = client.get_dataset('np')\n",
    "pd = client.get_dataset('pd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = text.replace(\",\", \" \").replace(\":\", \" \")\n",
    "    text = str(text).lower()\n",
    "    printable = set(string.printable)\n",
    "    text = \"\".join(list(filter(lambda x: x in printable, text)))\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in string.punctuation]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return \" \".join([word for word in tokens if word not in stopwords_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substract_words(query, words_list):\n",
    "    tokens = word_tokenize(query)\n",
    "    return \" \".join([token for token in tokens if token not in words_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_retrieval(query):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Algorithm:\n",
    "    \n",
    "    Step 1: Filter and Preprocess the query.\n",
    "    Step 2: Initialize result = []\n",
    "    Step 3: Read all the models.\n",
    "    Step 4: Disproportionate the query into project, libs, frameworks, dbs, keywords.\n",
    "    Step 3: Add the documents in the resultset if query keywords found in project title.\n",
    "    Step 4: If more than 70% of the query found in project title and technologies used in project then find cosine_similarity\n",
    "            between the query and the feature matrix. Return files with minimum distance.\n",
    "    Step 5: Else\n",
    "            a) Predict Topic from LDA model get documents with similar topics.\n",
    "            b) Find cosine_similarity with bert_embeddings with given query.\n",
    "            c) Sort and aggregate the results and return documents.\n",
    "    Step 6: Calculate Precision, Recall, F-Score for both scenario to tune the parameters.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1\n",
    "    query = clean(query)\n",
    "    query_len = len(word_tokenize(query))\n",
    "    \n",
    "    if query_len == 0:\n",
    "        return\n",
    "    \n",
    "    \n",
    "    # Step 2\n",
    "    result = pd.DataFrame()\n",
    "    query_data = {\n",
    "        \"project\": 0,\n",
    "        \"libs\": 0,\n",
    "        \"frameworks\": 0,\n",
    "        \"dbs\": 0\n",
    "    }\n",
    "    \n",
    "    # Step 3: \n",
    "    tf_projects = client.get_dataset('tf_projects')\n",
    "    tf_libs = client.get_dataset('tf_libs')\n",
    "    tf_frameworks = client.get_dataset('tf_frameworks')\n",
    "    tf_dbs = client.get_dataset('tf_dbs')\n",
    "    \n",
    "    # Step 4:\n",
    "    tf_project_query = tf_projects.transform([query]).toarray()\n",
    "    tf_project_query = np.nonzero(tf_project_query)[1]\n",
    "    query_data['project'] = len(tf_project_query) / query_len\n",
    "    \n",
    "    tf_libs_query = tf_libs.transform([query]).toarray()\n",
    "    tf_libs_query = np.nonzero(tf_libs_query)[1]\n",
    "    query_data['libs'] = len(tf_libs_query) / query_len\n",
    "    if query_data['libs'] > 0:\n",
    "        tf_libs_query_words = np.array(tf_libs.get_feature_names())[tf_libs_query]\n",
    "        query = substract_words(query, tf_libs_query_words)    \n",
    "    \n",
    "    tf_frameworks_query = tf_frameworks.transform([query]).toarray()\n",
    "    tf_frameworks_query = np.nonzero(tf_frameworks_query)[1]\n",
    "    query_data['frameworks'] = len(tf_frameworks_query) / query_len\n",
    "    if query_data['frameworks'] > 0:\n",
    "        tf_frameworks_query_words = np.array(tf_frameworks.get_feature_names())[tf_frameworks_query]\n",
    "        query = substract_words(query, tf_frameworks_query_words)\n",
    "    \n",
    "    tf_dbs_query = tf_dbs.transform([query]).toarray()\n",
    "    tf_dbs_query = np.nonzero(tf_dbs_query)[1]\n",
    "    query_data['dbs'] = len(tf_dbs_query) / query_len\n",
    "    if query_data['dbs'] > 0:\n",
    "        tf_dbs_query_words = np.array(tf_dbs.get_feature_names())[tf_dbs_query]\n",
    "        query = substract_words(query, tf_dbs_query_words)\n",
    "    \n",
    "    # Step 5:\n",
    "    if query_data['libs'] > 0:\n",
    "        tf_libs_matrix = client.get_dataset('tf_libs_matrix')\n",
    "        tf_libs_distance = dd.from_dask_array(da.from_array(euclidean_distances(tf_libs.transform([query]).toarray(), tf_libs_matrix.drop([\"project\", \"file\"], axis=1).to_dask_array()).compute().reshape(-1, 1)), columns=[\"distance\"])\n",
    "        tf_libs_distance['project'] = tf_libs_matrix['project']\n",
    "        tf_libs_distance['file'] = tf_libs_matrix['file']\n",
    "        tf_libs_distance = tf_libs_distance.nsmallest(5, \"distance\").compute()[['project', 'file']]\n",
    "        result = pd.concat([result, tf_libs_distance])\n",
    "        \n",
    "    \n",
    "    if query_data['frameworks'] > 0:\n",
    "        tf_frameworks_matrix = client.get_dataset('tf_frameworks_matrix')\n",
    "        tf_frameworks_distance = dd.from_dask_array(da.from_array(euclidean_distances(tf_frameworks.transform([query]).toarray(), tf_frameworks_matrix.drop([\"project\", \"file\"], axis=1).to_dask_array()).compute().reshape(-1, 1)), columns=[\"distance\"])\n",
    "        tf_frameworks_distance['project'] = tf_frameworks_matrix['project']\n",
    "        tf_frameworks_distance['file'] = tf_frameworks_matrix['file']\n",
    "        tf_frameworks_distance = tf_frameworks_distance.nsmallest(5, \"distance\").compute()[['project', 'file']]\n",
    "        result = pd.concat([result, tf_frameworks_distance])\n",
    "        \n",
    "        \n",
    "    if query_data['dbs'] > 0:\n",
    "        tf_dbs_matrix = client.get_dataset('tf_dbs_matrix')\n",
    "        tf_dbs_distance = dd.from_dask_array(da.from_array(euclidean_distances(tf_projects.transform([query]).toarray(), tf_dbs_matrix.drop([\"project\", \"file\"], axis=1).to_dask_array()).compute().reshape(-1, 1)), columns=[\"distance\"])\n",
    "        tf_dbs_distance['project'] = tf_dbs_matrix['project']\n",
    "        tf_dbs_distance['file'] = tf_dbs_matrix['file']\n",
    "        tf_dbs_distance = tf_dbs_distance.nsmallest(5, \"distance\").compute()[['project', 'file']]\n",
    "        result = pd.concat([result, tf_dbs_distance])\n",
    "        \n",
    "        \n",
    "    if query_data['project'] > 0:\n",
    "        tf_project_matrix = client.get_dataset('tf_project_matrix')\n",
    "        tf_project_distance = dd.from_dask_array(da.from_array(euclidean_distances(tf_projects.transform([query]).toarray(), tf_project_matrix.drop([\"project\", \"file\"], axis=1).to_dask_array()).compute().reshape(-1, 1)), columns=[\"distance\"])\n",
    "        tf_project_distance['project'] = tf_project_matrix['project']\n",
    "        tf_project_distance['file'] = tf_project_matrix['file']\n",
    "        tf_project_distance = tf_project_distance.nsmallest(5, \"distance\").compute()[['project', 'file']]\n",
    "        result = pd.concat([result, tf_project_distance])\n",
    "        \n",
    "        \n",
    "    if len(word_tokenize(query)) > 0:\n",
    "        \n",
    "        id2word = client.get_dataset('id2word')\n",
    "        lda_model = client.get_dataset('lda_model')\n",
    "        context_based_feature_matrix = client.get_dataset('context_based_feature_matrix')\n",
    "        \n",
    "        corpus = id2word.doc2bow(word_tokenize(query))\n",
    "        topics_distribution = dict(lda_model[corpus][0])\n",
    "        topic = sorted(topics_distribution.items(), key=lambda x: x[1], reverse=True)[0][0]\n",
    "        context_based_topic_result = context_based_feature_matrix[context_based_feature_matrix['Dominant_Topic'] == topic]\n",
    "        \n",
    "        \n",
    "        query_vec = bert.encode([query])\n",
    "        context_based_result = dd.from_dask_array(da.from_array(euclidean_distances(query_vec, context_based_topic_result.drop([\"project\", \"file_y\", \"keyword\", \"Dominant_Topic\", \"Topic_Perc_Contrib\"], axis=1).to_dask_array()).compute()).reshape(-1, 1), columns=[\"distance\"])\n",
    "        context_based_result.index = context_based_topic_result.index\n",
    "        context_based_result['project'] = context_based_topic_result['project']\n",
    "        context_based_result['file'] = context_based_topic_result['file_y']\n",
    "        \n",
    "        context_based_result = context_based_result.nsmallest(25, 'distance').groupby(['project', 'file']).agg({'distance': sum}).reset_index().nsmallest(10, 'distance').compute()[['project', 'file']]\n",
    "        result = pd.concat([result, context_based_result])\n",
    "    \n",
    "    \n",
    "    result = result.drop_duplicates()\n",
    "    return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.26 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>network edge computing device</td>\n",
       "      <td>./reports_doc/NETWORK EDGE COMPUTING DEVICE_35...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>transparent distributed computing for cuda of ...</td>\n",
       "      <td>./reports_doc/Transparent Distributed Computin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>volunteer computing for high performance vcom</td>\n",
       "      <td>./reports_doc/vCom-Volunteer Computing for Hig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>analysis of videos using deep learning and hig...</td>\n",
       "      <td>./reports_doc/Analysis of Videos using Deep Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>insider threat detection and adaptive two fact...</td>\n",
       "      <td>./reports_doc/INSIDER THREAT DETECTION AND_ADA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>autoscaling of application servers</td>\n",
       "      <td>./reports/13_final_report.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bblock data protection appliance</td>\n",
       "      <td>./reports/15_final_report.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cloudbucket</td>\n",
       "      <td>./reports/03_final_report.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cross cloud disaster recovery</td>\n",
       "      <td>./reports/BE Project Report_6.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dynamic load balancing of web servers</td>\n",
       "      <td>./reports_doc/Dynamic load balancing of web se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hosting server on android phones using nano-ht...</td>\n",
       "      <td>./reports_doc/Creating server on android phone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>metering and throttling for remedy applications</td>\n",
       "      <td>./reports_doc/Metering and Throttling for Reme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>openstack resource management tool</td>\n",
       "      <td>./reports_doc/OpenStack Resource Management To...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>physical web with vending machine</td>\n",
       "      <td>./reports_doc/PHYSICAL WEB WITH VENDING MACHIN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>enhanced framework for true sight network auto...</td>\n",
       "      <td>./reports_doc/ENHANCED FRAMEWORK FOR TRUE_SIGH...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               project  \\\n",
       "300                      network edge computing device   \n",
       "400  transparent distributed computing for cuda of ...   \n",
       "404      volunteer computing for high performance vcom   \n",
       "37   analysis of videos using deep learning and hig...   \n",
       "263  insider threat detection and adaptive two fact...   \n",
       "0                   autoscaling of application servers   \n",
       "2                     bblock data protection appliance   \n",
       "4                                          cloudbucket   \n",
       "5                        cross cloud disaster recovery   \n",
       "7                dynamic load balancing of web servers   \n",
       "9    hosting server on android phones using nano-ht...   \n",
       "11     metering and throttling for remedy applications   \n",
       "12                  openstack resource management tool   \n",
       "13                   physical web with vending machine   \n",
       "8    enhanced framework for true sight network auto...   \n",
       "\n",
       "                                                  file  \n",
       "300  ./reports_doc/NETWORK EDGE COMPUTING DEVICE_35...  \n",
       "400  ./reports_doc/Transparent Distributed Computin...  \n",
       "404  ./reports_doc/vCom-Volunteer Computing for Hig...  \n",
       "37   ./reports_doc/Analysis of Videos using Deep Le...  \n",
       "263  ./reports_doc/INSIDER THREAT DETECTION AND_ADA...  \n",
       "0                        ./reports/13_final_report.pdf  \n",
       "2                        ./reports/15_final_report.pdf  \n",
       "4                        ./reports/03_final_report.pdf  \n",
       "5                    ./reports/BE Project Report_6.pdf  \n",
       "7    ./reports_doc/Dynamic load balancing of web se...  \n",
       "9    ./reports_doc/Creating server on android phone...  \n",
       "11   ./reports_doc/Metering and Throttling for Reme...  \n",
       "12   ./reports_doc/OpenStack Resource Management To...  \n",
       "13   ./reports_doc/PHYSICAL WEB WITH VENDING MACHIN...  \n",
       "8    ./reports_doc/ENHANCED FRAMEWORK FOR TRUE_SIGH...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "information_retrieval(\"cloud-computing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
